{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/nq/8n0hp2s11l7066_mjdy4c8jm0000gp/T/ipykernel_11484/338357025.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mtraining_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcategory\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mCATEGORIES\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATADIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcategory\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# path to cats or dogs dir\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m         \u001B[0mclass_num\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCATEGORIES\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcategory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mimg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# read training images\n",
    "DATADIR = \"./train/\"\n",
    "#CATEGORIES = os.listdir(DATADIR)\n",
    "CATEGORIES = [\"Closed\", \"Open\"]\n",
    "IMG_SIZE = 100\n",
    "\n",
    "training_data = []\n",
    "for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category) # path to cats or dogs dir\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE) # colour is not specific in this use-case\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X = [] # features\n",
    "y = [] # labels\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # -1 is default, last 1 for grey scale\n",
    "y = np.array(y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drowsiness_Detection_Model_0_3_conv_16_nodes_1_dense_1653480345\n",
      "Train on 1161 samples, validate on 291 samples\n",
      "Epoch 1/5\n",
      "1161/1161 [==============================] - 3s 3ms/sample - loss: 0.6481 - accuracy: 0.6593 - val_loss: 0.1391 - val_accuracy: 0.9502\n",
      "Epoch 2/5\n",
      "1161/1161 [==============================] - 2s 2ms/sample - loss: 0.1422 - accuracy: 0.9526 - val_loss: 0.2243 - val_accuracy: 0.9347\n",
      "Epoch 3/5\n",
      "1161/1161 [==============================] - 3s 2ms/sample - loss: 0.1183 - accuracy: 0.9604 - val_loss: 0.1543 - val_accuracy: 0.9536\n",
      "Epoch 4/5\n",
      "1161/1161 [==============================] - 2s 2ms/sample - loss: 0.1172 - accuracy: 0.9612 - val_loss: 0.2086 - val_accuracy: 0.9485\n",
      "Epoch 5/5\n",
      "1161/1161 [==============================] - 2s 2ms/sample - loss: 0.0875 - accuracy: 0.9750 - val_loss: 0.0971 - val_accuracy: 0.9605\n",
      "Drowsiness_Detection_Model_1_5_conv_16_nodes_1_dense_1653480358\n",
      "Train on 1161 samples, validate on 291 samples\n",
      "Epoch 1/5\n",
      "1161/1161 [==============================] - 3s 3ms/sample - loss: 0.3718 - accuracy: 0.8480 - val_loss: 0.2173 - val_accuracy: 0.9296\n",
      "Epoch 2/5\n",
      "1161/1161 [==============================] - 3s 2ms/sample - loss: 0.1541 - accuracy: 0.9462 - val_loss: 0.1023 - val_accuracy: 0.9622\n",
      "Epoch 3/5\n",
      "1161/1161 [==============================] - 3s 2ms/sample - loss: 0.1606 - accuracy: 0.9475 - val_loss: 0.1508 - val_accuracy: 0.9433\n",
      "Epoch 4/5\n",
      "1161/1161 [==============================] - 3s 2ms/sample - loss: 0.1074 - accuracy: 0.9638 - val_loss: 0.1568 - val_accuracy: 0.9278\n",
      "Epoch 5/5\n",
      "1161/1161 [==============================] - 3s 2ms/sample - loss: 0.0783 - accuracy: 0.9711 - val_loss: 0.0494 - val_accuracy: 0.9811\n",
      "Drowsiness_Detection_Model_2_3_conv_32_nodes_1_dense_1653480371\n",
      "Train on 1161 samples, validate on 291 samples\n",
      "Epoch 1/5\n",
      "1161/1161 [==============================] - 6s 5ms/sample - loss: 0.4600 - accuracy: 0.8170 - val_loss: 0.1760 - val_accuracy: 0.9399\n",
      "Epoch 2/5\n",
      "1161/1161 [==============================] - 6s 5ms/sample - loss: 0.2460 - accuracy: 0.9268 - val_loss: 0.1935 - val_accuracy: 0.9502\n",
      "Epoch 3/5\n",
      "1161/1161 [==============================] - 6s 5ms/sample - loss: 0.1544 - accuracy: 0.9453 - val_loss: 0.1601 - val_accuracy: 0.9502\n",
      "Epoch 4/5\n",
      "1161/1161 [==============================] - 7s 6ms/sample - loss: 0.1107 - accuracy: 0.9599 - val_loss: 0.0933 - val_accuracy: 0.9674\n",
      "Epoch 5/5\n",
      "1161/1161 [==============================] - 7s 6ms/sample - loss: 0.1058 - accuracy: 0.9651 - val_loss: 0.1081 - val_accuracy: 0.9605\n",
      "Drowsiness_Detection_Model_3_5_conv_32_nodes_1_dense_1653480404\n",
      "Train on 1161 samples, validate on 291 samples\n",
      "Epoch 1/5\n",
      "1161/1161 [==============================] - 5s 4ms/sample - loss: 0.3585 - accuracy: 0.8325 - val_loss: 0.1349 - val_accuracy: 0.9502\n",
      "Epoch 2/5\n",
      "1161/1161 [==============================] - 4s 4ms/sample - loss: 0.1253 - accuracy: 0.9565 - val_loss: 0.1603 - val_accuracy: 0.9399\n",
      "Epoch 3/5\n",
      "1161/1161 [==============================] - 4s 4ms/sample - loss: 0.1124 - accuracy: 0.9630 - val_loss: 0.1100 - val_accuracy: 0.9674\n",
      "Epoch 4/5\n",
      "1161/1161 [==============================] - 4s 4ms/sample - loss: 0.0820 - accuracy: 0.9789 - val_loss: 0.0662 - val_accuracy: 0.9845\n",
      "Epoch 5/5\n",
      "1161/1161 [==============================] - 4s 4ms/sample - loss: 0.0757 - accuracy: 0.9737 - val_loss: 0.0986 - val_accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 4\n",
    "\n",
    "# compare different architectures\n",
    "dense_layers = [1]\n",
    "layer_sizes = [16, 32]\n",
    "conv_layers = [3, 5]\n",
    "\n",
    "model = [Sequential() for i in range(len(dense_layers)*len(layer_sizes)*len(conv_layers))]\n",
    "idx = 0\n",
    "\n",
    "for dense_layer in dense_layers: \n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            # analyse model\n",
    "            NAME = 'Drowsiness_Detection_Model_{}_{}_conv_{}_nodes_{}_dense_{}'.format(idx, conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            # input layer\n",
    "            model[idx].add(Conv2D(layer_size, (3,3), input_shape = X.shape[1:]))\n",
    "            model[idx].add(Activation('relu'))\n",
    "            model[idx].add(MaxPooling2D(pool_size = (2,2))) \n",
    "            \n",
    "            # hidden convulational layers (='filters')\n",
    "            for l in range(conv_layer-1):\n",
    "                model[idx].add(Conv2D(layer_size, (3,3)))\n",
    "                model[idx].add(Activation('relu'))\n",
    "                model[idx].add(MaxPooling2D(pool_size = (2,2)))\n",
    "            \n",
    "            # hidden dense (=fully connected) layers\n",
    "            model[idx].add(Flatten())\n",
    "            for l in range(dense_layer):\n",
    "                model[idx].add(Dense(layer_size))\n",
    "                model[idx].add(Activation('relu'))\n",
    "\n",
    "            # output layer\n",
    "            model[idx].add(Dense(len(CATEGORIES)))\n",
    "            model[idx].add(Activation('sigmoid'))\n",
    "\n",
    "            model[idx].compile(loss='binary_crossentropy',\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "            model[idx].fit(X, y, batch_size=4, epochs=5, validation_split = 0.2) # appropriate batch size depends on size of data set\n",
    "            idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model[0].save('drowsinessEyeCnn.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n",
      "eyes closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-06a44a532018>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Video Stream'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 33\u001B[1;33m     \u001B[1;32mif\u001B[0m \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwaitKey\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m&\u001B[0m \u001B[1;36m0xFF\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m27\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     34\u001B[0m         \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    \n",
    "    eyes = eye_cascade.detectMultiScale(image, minSize=(40,40), maxSize=(50,50))\n",
    "    grayFrame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    left = True\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        if left:\n",
    "            left_eye = grayFrame[ey : ey + eh, ex : ex + ew]\n",
    "            left_eye = cv2.resize(left_eye, (IMG_SIZE, IMG_SIZE))\n",
    "            cv2.rectangle(image, (ex,ey), (ex+ew, ey+eh), (0,255,0), 6)\n",
    "            cv2.imshow('Left Eye', left_eye)\n",
    "            left_eye = np.array(left_eye).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # -1 is default, last 1 for grey scale\n",
    "            pred = model[1].predict(left_eye)\n",
    "            left = False\n",
    "        else:\n",
    "            right_eye = grayFrame[ey : ey + eh, ex : ex + ew]\n",
    "            right_eye = cv2.resize(right_eye, (IMG_SIZE, IMG_SIZE))\n",
    "            cv2.rectangle(image, (ex,ey), (ex+ew, ey+eh), (0,255,0), 6)\n",
    "            cv2.imshow('Right Eye', right_eye)\n",
    "            right_eye = np.array(right_eye).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # -1 is default, last 1 for grey scale\n",
    "            pred = model[1].predict(right_eye)\n",
    "            left = True\n",
    "            \n",
    "        if(pred[0,0] > 0.9): \n",
    "            print('eyes closed')\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Video Stream', image)\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}